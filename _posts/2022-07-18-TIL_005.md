---
title: "[TIL_005] 2022-07-18"
excerpt: ""
categories:
  - TIL
tags:
last_modified_at: 2022-07-18
---

# 1. batch size / epoch / iteration
+ batch size
  + train dataset을 한 번에 신경망에 넣으면 학습시간도 오래 걸리고, 대용량의 데이터셋으로 인한 메모리 부족 문제도 발생할 수 있으므로 dataset을 소단위로 나누어 학습시킨다. 이때 dataset을 소단위에 속하는 data의 크기를 batch size라고 한다.
+ epoch
  + 전체 train dataset이 신경망을 통과한 횟수 (순전파+역전파)
+ iteration
  + 전체 train dataset이 1 epoch을 도는데 필요한 `mini-batch 갯수`
  + 즉, 1 epoch을 도는데 필요한 `parameters 업데이트 횟수`. (각 mini-batch마다 parameters 업데이트가 1번씩 진행된다)

![image01](/assets/images/2022-07-18-til_01.jpeg){: .align-center}{: width="60%" height="60%"}
<br>
![image02](/assets/images/2022-07-18-til_02.jpeg){: .align-center}{: width="60%" height="60%"}
*train dataset size = 800, batch size = 100, <br> 8 iterations for 1 epoch*

<br>

# 2. WandB (Weights & Biases) 툴
+ ML/DL 과정을 쉽게 tracking, hyperparameter tuning, 시각화할 수 있는 tool
+ 주요 기능
  + `Experiments`: experiment tracking (ML/DL 모델 실험 추적 Dashboard 제공)
  + `Artifacts`: dataset versioning (Dataset/Model version 관리)
  + `Tables`: model evaluation (output data를 loging하여 시각화하고 query하기 위한 기능)
  + `Sweeps`: model optimization (hyperparameter tuning을 통한 최적화)
  + `Reports`: Collaborative analysis (실험을 document로 정리 및 공유 기능)

<br>

# 3. Weight Decay
+ Training Data가 충분히 많다면 overfitting을 방지할 수 있지만, training data의 양이 충분하지 못할 때 overfitting이 발생한다.
이러한 상황에서 overfitting을 줄이는 기법 중 하나가 `weight decay`이다.
+ overfitting은 weight 매개변수의 값이 커서 발생하는 경우가 많다.
따라서, `Weight decay`는 training 과정에서 큰 weight에 대해 상응하는 패널티를 부과하여 overfitting을 억제하도록 해준다.
+ Weight decay에는 `Regularization`이 사용된다.
`Regularization`은 weight의 절대값을 가능한 작게 만들어준다. 즉, 기울기를 가능한 작게 만들어준다.
이로써 weight의 모든 원소를 0에 가깝게 하여 모든 특성이 output에 주는 영향을 최소한으로 만들어준다.
이렇게 regularization은 overfitting이 되지않도록 모델을 강제로 제한한다.

<br>

# 4. Hyperparameter Tuning

> Hyperparmeter tuning에는 3가지 방법이 있다. <br>
> 1) Grid Search <br>
> 2) Random Search <br>
> 3) Baysian Optimization

## 1) Grid Search
+ 사전에 탐색할 값들을 미리 지정하고, 그 값들의 모든 조합을 바탕으로 성능의 최고점을 찾아낸다.
+ 장점
  + 내가 원하는 파라미터의 조합 실험 가능
+ 단점
  + 시간이 오래 걸린다
  + 성능의 최고점이 아닐 가능성이 높다
  + 최적화 검색 o, 최적화 탐색 x
    + 최적화 검색: 여러개를 비교 분석해서 최고를 찾아내는 기법
    + 최적화 탐색: 성능이 가장 높은 점으로 점차 찾아가는 기법

## 2) Random Search
+ 사전에 탐색할 값들의 범위를 지정하고, 그 범위 속에서 가능한 조합을 바탕으로 성능의 최고점을 찾아낸다.
+ 장점
  + Grid Search에 비해 시간이 짧게 걸림
  + Grid Search보다 random하게 점을 찍으니 성능이 더 좋은 점으로 갈 가능성이 높음
+ 단점
  + 최적화 검색 o, 최적화 탐색 x

## 3) Bayesian Optimization
+ 사전에 탐색할 값들의 범위를 지정하고, random하게 R번 탐색한 후, B번만큼 최적의 값을 찾아간다.
+ 장점
  + 최적화 탐색 o (최적의 값을 찾아갈 수 있다)
+ 단점
  + random하게 찍은 값이 너무 부족하면, 최적의 값을 탐색하는게 불가능할 수 있다.
  + random하게 찍은 값이 너무 많으면, 최적화 이전에 이미 최적값을 가지고 있을 수도 있다.

<br>

# 5. `pickle` module 
+ [pickle posting](https://aijinsol.github.io/python/pickle/)

<br>

# Reference
+ [batch size, epoch, iteration](https://losskatsu.github.io/machine-learning/epoch-batch/#2-batch-size%EC%9D%98-%EC%9D%98%EB%AF%B8)
+ [weight decay](https://goatlab.tistory.com/124)
+ [hyperparmater tuning - grid search / random search / bayesian optimization](https://dacon.io/competitions/open/235698/talkboard/403915?page=1&dtype=recent&ptype&fType)

<br>
